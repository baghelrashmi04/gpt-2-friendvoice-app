{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681acb07-7d68-4956-9a43-2ece49dcee2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "result = classifier(\"I am feeling like nobody needs me\")\n",
    "print(result)\n",
    "\n",
    "generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "generated_text = generator(\"The cat exam for mba program \", max_length=20, num_return_sequences=1)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb6c78b-eb8f-4183-bb73-a4f0956df8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "# Try positive statements\n",
    "positive_result = classifier(\"This is a fantastic day!\")\n",
    "print(f\"Positive: {positive_result}\")\n",
    "\n",
    "positive_result_2 = classifier(\"I am feeling very happy and excited.\")\n",
    "print(f\"Positive 2: {positive_result_2}\")\n",
    "\n",
    "# Try negative statements\n",
    "negative_result = classifier(\"I am so sad and disappointed.\")\n",
    "print(f\"Negative: {negative_result}\")\n",
    "\n",
    "negative_result_2 = classifier(\"This is terrible and a waste of time.\")\n",
    "print(f\"Negative 2: {negative_result_2}\")\n",
    "\n",
    "# Try neutral or ambiguous statements\n",
    "neutral_result = classifier(\"The weather is cloudy today.\")\n",
    "print(f\"Neutral: {neutral_result}\")\n",
    "\n",
    "ambiguous_result = classifier(\"The movie was okay, I guess.\")\n",
    "print(f\"Ambiguous: {ambiguous_result}\")\n",
    "\n",
    "# Try your original statement again\n",
    "original_result = classifier(\"nobody needs me\")\n",
    "print(f\"Original: {original_result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7530810-772e-4e78-b32d-1f02c68f61cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "statements = [\n",
    "    \"This is a fantastic day!\",\n",
    "    \"I am feeling very happy and excited.\",\n",
    "    \"I am so sad and disappointed.\",\n",
    "    \"This is terrible and a waste of time.\",\n",
    "    \"The weather is cloudy today.\",\n",
    "    \"The movie was okay, I guess.\",\n",
    "    \"nobody needs me\"\n",
    "]\n",
    "\n",
    "results = []\n",
    "for statement in statements:\n",
    "    prediction = classifier(statement)[0] # The pipeline returns a list of dictionaries\n",
    "    results.append({\"statement\": statement, \"label\": prediction[\"label\"], \"score\": prediction[\"score\"]})\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b887ddf0-c65f-45f7-a6c5-28eea65257b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator= pipeline(\"text-generation\", model=\"gpt2\")\n",
    "prompt1= \"hello rashmi how are you\"\n",
    "generationtext1 = generator(prompt1, max_length=100 , num_return_sequences=1)\n",
    "print(generationtext1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be60b6d3-274f-4e17-9c3f-4d1fe6c164fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=\"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\")\n",
    "\n",
    "\n",
    "\n",
    "completion = client.completions.create(engine=\"davinci-003\",\n",
    "prompt=\"Translate 'Hello, how are you?' to French.\",\n",
    "max_tokens=50)\n",
    "print(completion.choices[0].text)\n",
    "\n",
    "chat_completion = client.chat.completions.create(model=\"gpt-3.5-turbo\",\n",
    "messages=[{\"role\": \"user\", \"content\": \"What is the capital of France?\"}])\n",
    "print(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22f713c0-9b6a-47ee-a5c0-1d5d90facae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1 posts.\n",
      "First few posts:\n",
      "---\n",
      "I help people and organizations build systems—for execution, learning, and long-term growth. I’m the founder of HIVE, a venture studio and apprenticeship program that prepares young professionals for careers built on independence, not dependence. My work blends product thinking, education, and systems design to help people do more of what matters—with clarity, agency, and purpose.These kind of posts make me the happiest and most proud. started working with Human Insight Ventures two months ago. In that time she has built her own system for getting things done, learning things, and sharing her work. No one is asking her to do this work. There is no assignment or pressure. She is coming at this from a place of curiosity and interest. Helping people become independant professionals, and setting them on the path to doing great work is the reason exists, and it is in posts like that I see proof that we are doing something write. Can't wait to see more from you , and all the other interns and apprentices at an intern at Human Insight Ventures sharing a prediction engine she's built on a model she's trained herself.Her curiosity and perseverance in learning about foundational models is more than a little inspiring to me. Can't wait to see all the amazing things she's going to buildLearn more about our program below if you're interested finding your place in tech and building an AI proof career! Becoming Independent in an AI-Driven WorldThe real risk for students isn’t that AI will take their jobs—it’s that they’ll be unprepared for a world without clear rules.For decades, the path was simple: get a degree, land a job, work your way up. But that path is changing fast. AI is reshaping the landscape—entry-level jobs are vanishing, grunt work is automated, and career growth is being rewritten.The truth? No one else has the answers eitherNot your college. Not your employer. In a world where the ground is shifting beneath your feet, the only way forward is to become independent—to build a career that doesn’t rely on a disappearing path.Dependency is a TrapMost people still believe that employers have the answers—that if you just get a good job, everything will be fine.But most companies are as lost as you are. They’re still figuring out what AI means for their business—let alone your career.Relying on employers to guide you is a risk—especially when they’re just as uncertain.Waiting for someone else to tell you what skills to build is like rearranging deck chairs on a sinking ship.Agency: The Only Advantage That MattersIn a world where AI can automate skills faster than you can learn them, the real advantage isn’t knowing what to do—it’s deciding what to do.Agency is about making decisions boldly—what skills to build, what tools to use, when to pivot.Thriving isn’t about how many technical skills you have—it’s about acting independently and using AI as a force multiplier, not a threat.Execution: Your Plan vs. No PlanHaving no plan is worse than having a bad one.Execution is about more than just doing the work—it’s about setting priorities and refusing to waste time on skills that won’t matter soon.It’s about turning ideas into outcomes—without waiting for permission.Playing it safe isn’t safe anymore—it’s just a slower way to fall behind.AI Changes the Game—But Not the RulesAI will rewrite job descriptions, automate tasks, and shift entire industries. But the fundamental rule of careers hasn’t changed:Those who control their path win. Those who wait for instructions lose.The only winning move is to build independence—before it’s too late.Stop Playing Defense Own Your PathA lot of people are still playing it safe—holding on to old skills and hoping the old path will come back.But playing it safe is just a slower way to lose.The only way to win is to go on offense—to build skills that AI can’t replace, to take control of your learning, and to make decisions without waiting for permission.Join HIVE’s Builder Apprenticeship ProgramWant to build the skills that make you independent Join HIVE’s Builder Apprenticeship Program and learn how to take control of your career—before someone else does.This isn’t about surviving AI’s impact—it’s about leveraging it.Learn more at One upside of helping people build work systems is that one's own systems are constantly being upgraded. A big part of the Human Insight Ventures Apprenticeship program is the notebook that all apprentices keep. Here we log deep work sessions, the key KPI in the program, and keep a track of everything we've done through the week and how engaging / energizing it felt, and do our end of week reviews. We ask apprentices to share their notebooks as a way to tell the world what they're working on, and this is something I don't do well. This week I worked on delivering the apprenticeship to the ~15 people we have in the program right now, worked on the website - humaninsightventures.com, and completed collaterals to go with an email for companies and universities looking for systems building instruction for their students / employees. Its scary to share this, but I can't ask people to do what I'm not doing myself, so look forward to a quick recap of my week towards the end of it going forward.If you are looking for training for your students or employees, reach out, I'd love to chat about all we've been working on I've been building out HIVEs career systems course over the last year, working with young people to help them build the foundations for successful careers.Once a very bright young person asked me what the KPI was for the course.I already had thought of one - the number of deep work hours we got in every week.As I've been writing out the modules for the course though, I've realized this is actually really really key. f we think about the primary drivers of why we do work, one big reason is because there are externally imposed consequences of not doing it. It starts out like - if you don't do your homework, you'll get told off in class. As we grow older, it changes into a lot of other things - exams failed, promotions skipped, jobs lost. If we work this way, the emotion that's driving is some flavor of fear. Many of us get used to working this way, and eventually we know no other way to work. Contrast this with doing deep work that get's one into a flow state. Where the time spent working seems not to exist, and that leaves one joyful, and fulfilled, and at serene ease. I've had a week of flow states and deep work. Its the best. But I'm so annoyed that its taken me 10+ years of working to figure this out. What a difference it would make if we could help people live this in their early professional lives. If we could socialize them in a work philosophy grounded in curiosity and meaning and joy. What a difference.An excerpt from the module I'm working on - So Deep work is doing cognitively challenging work in a way that we are experiencing flow states. Making this change, from doing cognitively work out of panic and fear, at the very last moment, to being able to plan and execute this kind of work in a way that becomes satisfying, is the foundation of being able to great work throughout your career. Because once you enjoy it, you can do more of it. In most situations, people only do cognitively challenging work when others force them to. When your boss or teacher asks you to.Once you start enjoying it, and get better at it, you can do it as much as you please.Once you enjoy doing deep work, you have a super power. The stuff that people will procrastinate till the last moment, you can do far in advance. And while people will do the very least amount of challenging work they can get away with, you can seek out challenging work and do more and more of it. Apart from bringing professional achievements, this will make your working life a pleasure. If you're interested in this kind of thinking work - you can see the modules I'm working on here Hi everyone I have a quick ask I’m planning to do a talk for students / recent grads / people looking for career direction in New Delhi.I have some ideas on topics that might be relevant, but want to talk to some people in this phase of life and get their feedback.If you or someone you know would be up for a quick chat to go some ideas for the talk, could you DM or comment here please? The big challenge for future generations will be how to use AI tools to enhance our skill sets, rather as a crutch at cost of the abilities that we have.Hi folks,I'm looking for a Chief of Staff to help build Human Insight Ventures. Ideally someone young - a year or two out of of University? More importantly, someone who is unreasonably curious and driven. And fun. D If you know any such people based in Delhi, please, please, send them my way?Thanks Devansh In light of the recent NEET exam controversy in India, which has highlighted the need for systemic reforms in education and public recruitment, Hive offers a unique approach to creating positive change.The NEET exam fiasco has exposed the vulnerabilities of India's highly competitive medical entrance system, with allegations of paper leaks, irregularities, and an unprecedented surge in high scores. This has left many aspiring students feeling disheartened and uncertain about their future.Our mission at Hive is to develop empathetic and effective individuals who can drive meaningful transformation, making students impact-proof from issues like the NEET exam controversyWe believe that by investing in personal growth and eAt Hive, we focus onCultivating empathy and emotional intelligence to foster compassionate leadership and decision-making. Developing practical skills and strategies for effective problem-solving and innovation.Encouraging independent thinking and entrepreneurial mindsets to navigate challenges and create positive change.Building a network of like-minded individuals committed to making a difference in their communities and beyond.When I moved back to Delhi 2 years ago from SF, the most glaring difference was the lack of a tech community here. This event is our attempt to bring together the vibrant ecosystem of founders and operators that exists in Delhi, so that no one needs to build alone Please do join us if you're around, and a tech founder or operator based in the capital! I've spoken to many people about building a venture studio focusing on career development and training. They've all told me it's a bad idea. There's no money in trainingYoung people are only interested in getting their first job, no one wants to invest in their careerVenture studios are losing propositions, theres no way to scale your attention\" I think theyre wrong. still believe helping young people build strong foundations for a career is the greatest impact I can make in the world. And I believe that one can do that in a profitable and sustainable way. Heres the vision and strategy behind Human Insight Ventures, a venture studio that aims to create empathic and effective people as the most viable strategy for affecting long-term change in the world.I wanted to share a few professional updates Over the last year I’ve been working on launching a new product at an American enterprise retail company.I’m very proud of the work we did. Creating change within enterprise companies is quite challenging, but the scale of impact one can have is very exciting.But now that’s over, the product is live and others better suited than me are taking it forwardI’m not super clear on the terms of my non-disclosure so won’t name namesI am now resuming full-time work on Human Insight Ventures I am going to be based full-time in India, where I can see myself making the biggest impact, and where I have a team working with me. We have been building Kitaabnama, a reading platform to bring a personal reading coach to every child. Writing and receiving letters is one of the simplest and greatest joys I know. I created a service to make it easier for people who share this joy, but find it hard to make the time for writing and sending letters. Just tell us your message and well take care of the rest. Check it out, and let me know what you think! so this is all I have on the name of data\n"
     ]
    }
   ],
   "source": [
    "file_path = \"linkdin.txt\"  # Replace with the actual path to your file\n",
    "\n",
    "try:\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        text_data = f.read()\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file '{file_path}' was not found. Make sure the path is correct.\")\n",
    "    exit()\n",
    "\n",
    "# Split the text into individual posts based on your delimiter (e.g., blank lines)\n",
    "posts = [post.strip() for post in text_data.split('\\n\\n') if post.strip()]\n",
    "\n",
    "print(f\"Loaded {len(posts)} posts.\")\n",
    "print(\"First few posts:\")\n",
    "for i in range(min(3, len(posts))):\n",
    "    print(f\"---\\n{posts[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd688c54-81a2-4af1-a1a5-53cfd56f3980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokenized posts: 1\n",
      "First tokenized post (as IDs): [40, 1849, 16794, 661, 290, 5745, 1382, 3341, 960, 1640, 9706, 11, 4673, 11, 290, 890, 12, 4354, 3349, 13, 314, 447, 247, 76, 262, 9119, 286, 367, 9306, 11, 257, 13189, 8034, 290, 38523, 1056, 1430, 326, 25978, 1862, 11153, 329, 16179, 3170, 319, 10404, 11, 407, 21403, 13, 2011, 670, 32067, 1720, 3612, 11, 3707, 11, 290, 3341, 1486, 284, 1037, 661, 466, 517, 286, 644, 6067, 960, 4480, 16287, 11, 4086, 11, 290, 4007, 13, 4711, 1611, 286, 6851, 787, 502, 262, 49414, 290, 749, 6613, 13, 2067, 1762, 351, 5524, 39917, 41673, 734, 1933, 2084, 13, 554, 326, 640, 673, 468, 3170, 607, 898, 1080, 329, 1972, 1243, 1760, 11, 4673, 1243, 11, 290, 7373, 607, 670, 13, 1400, 530, 318, 4737, 607, 284]\n",
      "Length of the first tokenized post: 128\n",
      "Decoded first post: I help people and organizations build systems—for execution, learning, and long-term growth. I’m the founder of HIVE, a venture studio and apprenticeship program that prepares young professionals for careers built on independence, not dependence. My work blends product thinking, education, and systems design to help people do more of what matters—with clarity, agency, and purpose.These kind of posts make me the happiest and most proud. started working with Human Insight Ventures two months ago. In that time she has built her own system for getting things done, learning things, and sharing her work. No one is asking her to\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Choose the pre-trained model and its tokenizer\n",
    "model_name = \"distilgpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "\n",
    "# Tokenize each post in your list\n",
    "tokenized_posts = [tokenizer.encode(post, truncation=True, padding='max_length', max_length=128) for post in posts]\n",
    "\n",
    "print(f\"Number of tokenized posts: {len(tokenized_posts)}\")\n",
    "print(f\"First tokenized post (as IDs): {tokenized_posts[0]}\")\n",
    "print(f\"Length of the first tokenized post: {len(tokenized_posts[0])}\")\n",
    "\n",
    "# You can also decode the first tokenized post back to text to see what it looks like\n",
    "decoded_post = tokenizer.decode(tokenized_posts[0])\n",
    "print(f\"Decoded first post: {decoded_post}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc15156-9871-4d03-a014-ef00be9a0e5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a37d531-9fbd-47ff-b34b-6540fff006d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 1\n",
      "Shape of the first training example: torch.Size([128])\n",
      "First training example:\n",
      "tensor([   40,  1849, 16794,   661,   290,  5745,  1382,  3341,   960,  1640,\n",
      "         9706,    11,  4673,    11,   290,   890,    12,  4354,  3349,    13,\n",
      "          314,   447,   247,    76,   262,  9119,   286,   367,  9306,    11,\n",
      "          257, 13189,  8034,   290, 38523,  1056,  1430,   326, 25978,  1862,\n",
      "        11153,   329, 16179,  3170,   319, 10404,    11,   407, 21403,    13,\n",
      "         2011,   670, 32067,  1720,  3612,    11,  3707,    11,   290,  3341,\n",
      "         1486,   284,  1037,   661,   466,   517,   286,   644,  6067,   960,\n",
      "         4480, 16287,    11,  4086,    11,   290,  4007,    13,  4711,  1611,\n",
      "          286,  6851,   787,   502,   262, 49414,   290,   749,  6613,    13,\n",
      "         2067,  1762,   351,  5524, 39917, 41673,   734,  1933,  2084,    13,\n",
      "          554,   326,   640,   673,   468,  3170,   607,   898,  1080,   329,\n",
      "         1972,  1243,  1760,    11,  4673,  1243,    11,   290,  7373,   607,\n",
      "          670,    13,  1400,   530,   318,  4737,   607,   284])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "train_dataset = []\n",
    "block_size = 128  # Same as max_length for simplicity\n",
    "\n",
    "for post in tokenized_posts:\n",
    "    # Create chunks of block_size with a sliding window of block_size / 2\n",
    "    for i in range(0, len(post) - block_size + 1, block_size // 2):\n",
    "        chunk = post[i:i + block_size]\n",
    "        train_dataset.append(torch.tensor(chunk))\n",
    "\n",
    "# Now train_dataset is a list of PyTorch tensors, each of length block_size\n",
    "\n",
    "print(f\"Number of training examples: {len(train_dataset)}\")\n",
    "print(f\"Shape of the first training example: {train_dataset[0].shape}\")\n",
    "print(f\"First training example:\\n{train_dataset[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de52e4c5-bf04-4925-8649-4b4edae9f738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on device: cpu\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "# Load the pre-trained model\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Add the padding token to the model's vocabulary if it's not already there\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# Move the model to the GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "tokenized_posts = [tokenizer.encode(post, truncation=True, padding='max_length', max_length=128) for post in posts]\n",
    "\n",
    "\n",
    "print(f\"Model loaded on device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a69366e-de44-40fb-b053-9fd0b43a770a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer and training dataset are ready.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./output\",          # Directory to save checkpoints and logs\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,              # Number of training epochs (you can adjust this)\n",
    "    per_device_train_batch_size=8,   # Batch size per GPU/CPU (adjust based on your memory)\n",
    "    save_steps=100,                  # Save checkpoint every 100 steps\n",
    "    save_total_limit=2,              # Only keep the last 2 checkpoints\n",
    "    logging_steps=10,               # Log metrics every 10 steps\n",
    "    learning_rate=5e-5,            # Learning rate for the optimizer\n",
    "    report_to=\"none\"                # Don't report to any external tracking (for simplicity)\n",
    ")\n",
    "\n",
    "# Create a simple dataset class\n",
    "class TextDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {'input_ids': self.encodings[idx], 'labels': self.encodings[idx]}\n",
    "\n",
    "# Create the training dataset\n",
    "train_dataset = TextDataset(tokenized_posts)\n",
    "\n",
    "# Create the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset\n",
    ")\n",
    "\n",
    "print(\"Trainer and training dataset are ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c96de92a-3f73-4892-9861-20331f8adf59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:03, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning complete!\n"
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "\n",
    "print(\"Fine-tuning complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "afd72fe0-3976-4279-b325-3a9f1079ed2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['model.safetensors', 'rng_state.pth', 'optimizer.pt', 'config.json', 'scheduler.pt', 'generation_config.json', 'training_args.bin', 'trainer_state.json']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "model_path = '/Users/rashmibaghel/learning_om/output/checkpoint-3'\n",
    "print(os.listdir(model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00920e37-38e6-40bf-a494-2f1477d02077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"_num_labels\": 1,\n",
      "    \"activation_function\": \"gelu_new\",\n",
      "    \"architectures\": [\n",
      "        \"GPT2LMHeadModel\"\n",
      "    ],\n",
      "    \"attn_pdrop\": 0.1,\n",
      "    \"bos_token_id\": 50256,\n",
      "    \"embd_pdrop\": 0.1,\n",
      "    \"eos_token_id\": 50256,\n",
      "    \"id2label\": {\n",
      "        \"0\": \"LABEL_0\"\n",
      "    },\n",
      "    \"initializer_range\": 0.02,\n",
      "    \"label2id\": {\n",
      "        \"LABEL_0\": 0\n",
      "    },\n",
      "    \"layer_norm_epsilon\": 1e-05,\n",
      "    \"model_type\": \"gpt2\",\n",
      "    \"n_ctx\": 1024,\n",
      "    \"n_embd\": 768,\n",
      "    \"n_head\": 12,\n",
      "    \"n_inner\": null,\n",
      "    \"n_layer\": 6,\n",
      "    \"n_positions\": 1024,\n",
      "    \"reorder_and_upcast_attn\": false,\n",
      "    \"resid_pdrop\": 0.1,\n",
      "    \"scale_attn_by_inverse_layer_idx\": false,\n",
      "    \"scale_attn_weights\": true,\n",
      "    \"summary_activation\": null,\n",
      "    \"summary_first_dropout\": 0.1,\n",
      "    \"summary_proj_to_labels\": true,\n",
      "    \"summary_type\": \"cls_index\",\n",
      "    \"summary_use_proj\": true,\n",
      "    \"task_specific_params\": {\n",
      "        \"text-generation\": {\n",
      "            \"do_sample\": true,\n",
      "            \"max_length\": 50\n",
      "        }\n",
      "    },\n",
      "    \"torch_dtype\": \"float32\",\n",
      "    \"transformers_version\": \"4.51.3\",\n",
      "    \"use_cache\": true,\n",
      "    \"vocab_size\": 50257\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "config_path = '/Users/rashmibaghel/learning_om/output/checkpoint-3/config.json'\n",
    "with open(config_path, 'r') as f:\n",
    "    config_data = json.load(f)\n",
    "print(json.dumps(config_data, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fb52bd27-695e-4f0c-b32f-33b26e137d81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: 'Thinking about the future integrated with AI '\n",
      "\n",
      "Generated 1:\n",
      "Thinking about the future integrated with AI .\n",
      "In a world where technology is growing and it․s becoming so commonplace, we need to see more research into what can happen if every one of us has an opportunity for technological advancement as well. I am using some examples: Microsoft Azure computing platform – all across platforms based on this paradigm will be able to learn by harnessing their data from existing systems that have already been created but then coming up again in search of new solutions (and maybe even advanced) without needing any coding time or effort involved.\n",
      "---\n",
      "Generated 2:\n",
      "Thinking about the future integrated with AI vernacular.\n",
      "The new technology for teaching, and to make people smarter by their intelligence skills—a concept that could be used as a tool in our classrooms today — is now at its peak on GitHub! The open source software platform has already reached an impressive milestone thanks entirely only of being released earlier this year (it․s not yet even available) but many developers are working hard behind closed doors or making progress toward building autonomous systems worldwide around humans*. There's no doubt there will be great challenges ahead: How does it work? What if you do something similar then we can build robots capable of taking control of all kinds of tasks without having them programmed into anything like how they're doing; what would have been done differently than where your code was written when someone wrote 'Smartbot' so far [for] us?' Or why use Python-based tools such Asimantara instead of using any other language/language?] But some things need\n",
      "---\n",
      "Generated 3:\n",
      "Thinking about the future integrated with AI xting, we will need to make a difference here in order to create an understanding of how and what is needed for this project.\n",
      "We›s now ready to go back into that game: there are no more than 5 main factions (Lions) left today — they should be able to take control over it by them – even when you already have some power on your hands! We intend to implement 4 player armies from each side - all players must meet their goals while doing so; as soon as possible our entire army can fully play together before its completion becomes available across many different locations along the way. In other words, after five months since finishing out single-player maps or getting rid first time around one faction at once, let's build up enough new units just because another enemy has been defeated within two stages without killing someone else . The final goal? How far away doesn't matter if multiple enemies come next door during three parts :\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Config, GPT2LMHeadModel, AutoTokenizer\n",
    "import torch\n",
    "import os\n",
    "from safetensors.torch import load_file\n",
    "\n",
    "# Specify the local directory where your fine-tuned model is saved\n",
    "model_path = '/Users/rashmibaghel/learning_om/output/checkpoint-3'\n",
    "\n",
    "# Construct paths to the config and model files\n",
    "config_path = os.path.join(model_path, 'config.json')\n",
    "model_weights_path = os.path.join(model_path, 'model.safetensors')\n",
    "\n",
    "# Try loading the configuration using the specific config class\n",
    "try:\n",
    "    config = GPT2Config.from_json_file(config_path)\n",
    "except Exception as e:\n",
    "    print(f\"Error loading config: {e}\")\n",
    "    raise\n",
    "\n",
    "# Try instantiating the GPT2LMHeadModel\n",
    "try:\n",
    "    model = GPT2LMHeadModel(config)\n",
    "except Exception as e:\n",
    "    print(f\"Error creating model from config: {e}\")\n",
    "    raise\n",
    "\n",
    "# Try loading the model weights using safetensors with strict=False\n",
    "try:\n",
    "    model.load_state_dict(load_file(model_weights_path), strict=False)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "except Exception as e:\n",
    "    print(f\"Error loading state dict (safetensors): {e}\")\n",
    "    raise\n",
    "\n",
    "# Load the pre-trained gpt2 tokenizer directly\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading tokenizer: {e}\")\n",
    "    raise\n",
    "\n",
    "# Set the prompt\n",
    "prompt = \"Thinking about the future integrated with AI \"\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# Generate text\n",
    "output_sequences = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    max_length=200,\n",
    "    num_return_sequences=3,\n",
    "    temperature=0.8,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "    do_sample=True,\n",
    "    pad_token_id=tokenizer.eos_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id,\n",
    "    no_repeat_ngram_size=2,  # Prevent repetition of 2-word sequences\n",
    "    repetition_penalty=1.2 \n",
    ")\n",
    "\n",
    "# Decode and print\n",
    "print(f\"Prompt: '{prompt}'\\n\")\n",
    "for i, sequence in enumerate(output_sequences):\n",
    "    generated_text = tokenizer.decode(sequence, skip_special_tokens=True)\n",
    "    print(f\"Generated {i+1}:\\n{generated_text}\\n---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45fee89-7faa-42b6-a237-d4f27a1a349a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08055f61-2fd5-4c0b-bf9c-6191494c747b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
